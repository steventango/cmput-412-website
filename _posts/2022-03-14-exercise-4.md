---
layout: post
title:  Exercise 4 - Don’t Crash! Tailing Behaviour
redirect_from:
- /exercise-4
---

"It was just a fluke"


TODO: ● A video of your Duckiebot safely driving around the town behind another Duckiebot
○ Your video should show your Duckiebot taking a right hand turn and left hand turn at the intersections and drive at least two full laps of the town

Omg, check out [our video](), yay!


TODO: A short introduction (2-3 sentences)

This lab was a hard sprint right from the start. Despite the overall
implementation being much more obvious than the straightforward than all
previous labs, we experienced many difficulties stemming from the real-time
nature of this lab.

## Strategy

Our strategy centres around a state machine. We use the following states:

 - Lane Following: Runs lane following node. This is the default when we lose
   track of the bot ahead
 - Stopping: We approach a red line and come to a halt. While we wait, we look
   for a bot ahead of us to decide which way to turn
 - Tracking: We follow the robot in front of us
 - Blind movements: We override the lane following to force a turn/forward
   across the intersection to follow the bot based on where we observed it
   turning while stopped

Our state machine starts off lane following, then switches to tracking when it
sees the bot and back when it loses it. The stopping state takes precedence over
both of these states, so the bot will unconditionally stop when seeing a red
line. The blind states only last for a few seconds each, though also get a "stop
immunity" where the bot ignores all red lines. This is important when going
through an intersection, otherwise we'd stop in the middle of the intersection
by seeing the red line from the other line.

Check out the diagram below!

Our LEDs sort of indicate the state... though they're very delayed sometimes due
to duckiebot driver issues, so take them with a grain of salt. Blue only happens
when the bot stops or moves backward while tracking.

Aside from the contour-masking code for the yellow lane from the previous lab,
the new sensors in this one come from

 1. Detecting the grid on the back of another duckiebot
 2. Using the time-of-flight (TOF) sensor to detect distance

We initially struggled to get the TOF to integrate well with the distance
transform we were getting from the grid-tracking-camera node. For some reason
the two gave very different distances. To resolve this, we plotted them again
each other and fitted a line to the points. Using the slope, of this line, we
were able to convert between both measurements, allowing us to fuse both
sensors. We found TOF particularly important at close distances, where the
camera could no longer fully see the grid on the back. The camera provided a far
great field-of-view, which is exactly when the TOF lost distance measures, so
the two sensors were able to cover for the weaknesses of the other. When both
were sensing, we took a weighted average to get an even more accurate estimate.

Because the camera is intermittent in its publishing we use sensor fusion wiht a
linear function of the TOF range

TODO: A brief paragraph describing your implemented strategy for maintaining a safe driving
distance and avoided collisions

## Discussion

TODO: A brief paragraph discussing your results; please answer these questions:


### How well did your implemented strategy work?

TODO

### Was it reliable?

TODO

### In what situations did it perform poorly?

TODO

## Sources

 * TODO
