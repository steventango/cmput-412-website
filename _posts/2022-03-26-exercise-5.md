---
layout: post
title:  Exercise 5 - ML for Robotics
redirect_from:
- /exercise-5
---

## MNIST Dataset and ML basic terminologies

### Deliverable 1

<iframe
    width="100%"
    height="600"
    src="/cmput-412-website/images/exercise-5/deliverable-1-backprop.pdf"
    frameborder="0">
</iframe>

### Deliverable 2

### What data augmentation is used in training? Please delete the data augmentation and rerun the code to compare.

A random rotation of the image between (-5, 5) degrees, then add 2 pixels of
padding to the image, and then randomly cropping the image to 28 x 28 pixels.
Rerunning the code without data augmentation results in a lower test accuracy, of 97.91%
as opposed to 98.12% with data augmentation. This is because data augmentation
helps prevent overfitting by increasing the amount of variation in the training
data.

### What is the batch size in the code? Please change the batch size to 16 and 1024 and explain the variation in results.

The batch size in the code is 64. Changing the batch size to 16 results in a
slightly higher test accuracy of 98.13% as opposed to 98.12% with a batch size
of 64. Changing the batch size to 1024 results in a decreased test accuracy of
97.54%. This is because smaller batch sizes result in more stochastic gradients,
which can help the neural network escape local minima and find a better global
minimum.

### What activation function is used in the hidden layer? Please replace it with the linear activation function and see how the training output differs. Show your results before and after changing the activation function in your written report.

ReLU. Replacing it with the linear activation function results in only 87.02%
test accuracy. This is because the activation function has to be non-linear in
order to be able to learn a non-linear function. This is really evident in the
t-SNE plots below, where using the linear activation function leads to a model
that is unable to separate the classes well.

t-SNE before changing the activation function:
![t-SNE before](/cmput-412-website/images/exercise-5/t-SNE.avif)

t-SNE after changing the activation function:
![t-SNE after](/cmput-412-website/images/exercise-5/t-SNE-linear.avif)

### What is the optimization algorithm in the code? Explain the role of optimization algorithm in training process

Adam is an optimization algorithm used in the code. Adam is similar to SGD but
it computes adaptive learning rates for each parameter through exponential
moving averages of past gradients and the squared gradients. The role of the
optimization algorithm is to efficiently update the weights of the neural
network to minimize the loss function.

### Add dropout in the training and explain how the dropout layer helps in training

Adding dropout decreased the test accuracy to 97.13%, this is because
with dropout, the neural network requires more epochs to learn a good model
as some of the neurons are dropped during training and thus their weights are
not updated as much.

Dropout is a form of regularization that helps prevent overfitting by randomly
dropping neurons from the neural network during training, this forces the neural
network to learn more robust features.

## Number Detection Node

### Deliverable 3

TODO: A brief paragraph describing your implemented strategy for number detection using ML

TODO: A brief paragraph discussing your results; please answer these questions:

### How well did your implemented strategy work?

TODO

### Was it reliable?

TODO

### In what situations did it perform poorly?

TODO

## Sources

- [Pytorch MNIST example](https://github.com/pytorch/examples/blob/main/mnist/main.py)
